{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"data/songs.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)[1:-5]\n",
    "\n",
    "with open(\"data/songs.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    test_data = list(reader)[-5:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('openai-community/gpt2')\n",
    "# tokenizer.bos_token = '<s>'\n",
    "# tokenizer.eos_token = '</s>'\n",
    "# tokenizer.pad_token = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Union, Dict\n",
    "from a2_p1_murugan_116745378 import TrigramLM, get_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [tokenizer.tokenize(row[2]) for row in data]\n",
    "# TODO: check if there is an issue due to sequence length > 1024\n",
    "# TODO: check if custom newline handling is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(GPT2TokenizerFast):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # self.from_pretrained(\"openai-community/gpt2\")\n",
    "        self.add_special_tokens(\n",
    "            {\n",
    "                \"bos_token\": \"<s>\",\n",
    "                \"eos_token\": \"</s>\",\n",
    "                \"pad_token\": \"<|endoftext|>\",\n",
    "            }\n",
    "        )\n",
    "        self.add_special_tokens({\"pad_token\": \"<|endoftext|>\"})\n",
    "\n",
    "    def tokenize2(self, text: str) -> List[str]:\n",
    "        tokens = []\n",
    "        while len(text):\n",
    "            text1, text = text[:1024], text[1024:]\n",
    "            # NEED TO SPLIT AT WHITESPACE\n",
    "            tokens1 = super().tokenize(text1)\n",
    "            print(len(text1), len(tokens1))\n",
    "            tokens.extend(tokens1)\n",
    "        return tokens\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = CustomTokenizer.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.tokenize(data[0][2])\n",
    "b = tokenizer.tokenize2(data[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 283\n",
    "a[i], b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.tokenize2(data[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel = TrigramLM(tokenizer)\n",
    "lmodel.train([i[2] for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.unigram_count[\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|<endoftext>|\" in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.nextProb([\"I\"], [\"Ġremember\", \"Ġwhen\", \"ĠI\", \"Ġwas\", \"Ġyoung\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.nextProb([], [\"<s>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.nextProb([\"I\", \"Ġremember\"], [\"Ġwhen\", \"ĠI\", \"Ġwas\", \"Ġyoung\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.nextProb([\"<s>\", \"I\"], [\"Ġremember\", \"Ġwhen\", \"ĠI\", \"Ġwas\", \"Ġyoung\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = lmodel.get_sequence_probability([\"I\", \"Ġremember\", \"Ġwhen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perplexity(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2_p2_murugan_116745378 import RecurrentLM, process_data\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "print(tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token, tokenizer.unk_token)\n",
    "print(tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id, tokenizer.unk_token_id)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.bos_token = '<s>'\n",
    "tokenizer.eos_token = '</s>'\n",
    "tokenizer.pad_token = '<|endoftext|>'\n",
    "\n",
    "tokenizer.add_tokens([\"<s>\", \"</s>\"])\n",
    "\n",
    "print(tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token, tokenizer.unk_token)\n",
    "print(tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id, tokenizer.unk_token_id)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"<s>\" in tokenizer.vocab\n",
    "tokenizer.vocab[\"</s>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "token_ids = tokenizer.encode(data[0][2])\n",
    "\n",
    "chunk_len = 128 - 2\n",
    "pad_count = (chunk_len - len(token_ids) % chunk_len) if len(token_ids) % chunk_len != 0 else 0\n",
    "\n",
    "token_ids += [0] * pad_count\n",
    "chunked_token_ids = torch.tensor(token_ids).reshape(-1, chunk_len)\n",
    "\n",
    "bos_tensor = torch.full((chunked_token_ids.shape[0], 1), -1)\n",
    "eos_tensor = torch.full((chunked_token_ids.shape[0], 1), -2)\n",
    "\n",
    "torch.cat((bos_tensor, chunked_token_ids, eos_tensor), dim=1).shape\n",
    "\n",
    "chunked_token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_tokens(tokens, start_token_id, end_token_id, pad_token_id, chunk_len=128):\n",
    "    u_chunk_len = chunk_len - 2\n",
    "    # padding\n",
    "    pad_count = (u_chunk_len - len(tokens) % u_chunk_len) if len(tokens) % u_chunk_len != 0 else 0\n",
    "    tokens += [pad_token_id] * pad_count\n",
    "\n",
    "    # chunking\n",
    "    chunked_tokens = torch.tensor(tokens).reshape(-1, u_chunk_len)\n",
    "\n",
    "    # adding start and end tokens\n",
    "    bos_tensor = torch.full((chunked_tokens.shape[0], 1), start_token_id)\n",
    "    eos_tensor = torch.full((chunked_tokens.shape[0], 1), end_token_id)\n",
    "    chunks = torch.cat((bos_tensor, chunked_tokens, eos_tensor), dim=1)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunk_tokens(\n",
    "    tokenizer.encode(\"Hello, how are you?\"),\n",
    "    tokenizer.bos_token_id,\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.pad_token_id,\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "p_data = [re.sub(r'\\n\\[[\\x20-\\x7f]+\\]', \"\", row[2]) for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_tokens(tokenizer.encode(p_data[0]), tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id, 128).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.randn(3, 4)\n",
    "y = torch.tensor([1, 2, 3])\n",
    "# loss_fn = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=1)\n",
    "loss_fn(p, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentLM(len(tokenizer.vocab), 64, 1024)\n",
    "model.load_state_dict(torch.load(\"results/model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    \"And you gotta live with the bad blood now\",\n",
    "    \"Sit quiet by my side in the shade\",\n",
    "    \"And I'm not even sorry, nights are so starry\",\n",
    "    \"You make me crazier, crazier, crazier, oh\",\n",
    "    \"When time stood still and I had you\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "    p = tokenizer.encode(test_data[i])\n",
    "    p1 = [tokenizer.bos_token_id] + p[:-1]\n",
    "    p2 = p[1:]\n",
    "    p1 = torch.tensor(p1)\n",
    "    p2 = torch.tensor(p2)\n",
    "\n",
    "    logits, _ = model(p1)\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "    prob = torch.gather(probabilities, 1, p2.unsqueeze(1)).squeeze(1).tolist()\n",
    "    print(f\"Perplexity for '{test_data[i]}': {get_perplexity(prob):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import a2_p2_murugan_116745378\n",
    "importlib.reload(a2_p2_murugan_116745378)\n",
    "from a2_p2_murugan_116745378 import generate\n",
    "\n",
    "model.to(\"cuda\")\n",
    "generated_tokens = generate(model, tokenizer, \"<s>I'm hot\", 100, \"cuda\")\n",
    "\n",
    "# print(tokenizer.decode(generated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.distributions.Categorical(logits=logits[-1:]).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse538",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
