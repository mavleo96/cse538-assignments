{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"data/songs.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    data = [row for row in reader] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('openai-community/gpt2')\n",
    "tokenizer.bos_token = '<s>'\n",
    "tokenizer.eos_token = '</s>'\n",
    "tokenizer.pad_token = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Union, Dict\n",
    "from a2_p1_murugan_116745378 import TrigramLM, get_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [tokenizer.tokenize(row[2]) for row in data]\n",
    "# TODO: check if there is an issue due to sequence length > 1024\n",
    "# TODO: check if custom newline handling is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel = TrigramLM(tokenizer)\n",
    "lmodel.train([i[2] for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|<endoftext>|\" in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.nextProb([\"I\"], [\"Ġremember\", \"Ġwhen\", \"ĠI\", \"Ġwas\", \"Ġyoung\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.nextProb([], [\"<s>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.nextProb([\"I\", \"Ġremember\"], [\"Ġwhen\", \"ĠI\", \"Ġwas\", \"Ġyoung\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.nextProb([\"<s>\", \"I\"], [\"Ġremember\", \"Ġwhen\", \"ĠI\", \"Ġwas\", \"Ġyoung\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = lmodel.get_sequence_probability([\"I\", \"Ġremember\", \"Ġwhen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perplexity(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "print(tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token, tokenizer.unk_token)\n",
    "print(tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id, tokenizer.unk_token_id)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.bos_token = '<s>'\n",
    "tokenizer.eos_token = '</s>'\n",
    "tokenizer.pad_token = '<|endoftext|>'\n",
    "\n",
    "tokenizer.add_tokens([\"<s>\", \"</s>\"])\n",
    "\n",
    "print(tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token, tokenizer.unk_token)\n",
    "print(tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id, tokenizer.unk_token_id)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"<s>\" in tokenizer.vocab\n",
    "tokenizer.vocab[\"</s>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "token_ids = tokenizer.encode(data[0][2])\n",
    "\n",
    "chunk_len = 128 - 2\n",
    "pad_count = (chunk_len - len(token_ids) % chunk_len) if len(token_ids) % chunk_len != 0 else 0\n",
    "\n",
    "token_ids += [0] * pad_count\n",
    "chunked_token_ids = torch.tensor(token_ids).reshape(-1, chunk_len)\n",
    "\n",
    "bos_tensor = torch.full((chunked_token_ids.shape[0], 1), -1)\n",
    "eos_tensor = torch.full((chunked_token_ids.shape[0], 1), -2)\n",
    "\n",
    "torch.cat((bos_tensor, chunked_token_ids, eos_tensor), dim=1).shape\n",
    "\n",
    "chunked_token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_tokens(tokens, start_token_id, end_token_id, pad_token_id, chunk_len=128):\n",
    "    u_chunk_len = chunk_len - 2\n",
    "    # padding\n",
    "    pad_count = (u_chunk_len - len(tokens) % u_chunk_len) if len(tokens) % u_chunk_len != 0 else 0\n",
    "    tokens += [pad_token_id] * pad_count\n",
    "\n",
    "    # chunking\n",
    "    chunked_tokens = torch.tensor(tokens).reshape(-1, u_chunk_len)\n",
    "\n",
    "    # adding start and end tokens\n",
    "    bos_tensor = torch.full((chunked_tokens.shape[0], 1), start_token_id)\n",
    "    eos_tensor = torch.full((chunked_tokens.shape[0], 1), end_token_id)\n",
    "    chunks = torch.cat((bos_tensor, chunked_tokens, eos_tensor), dim=1)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunk_tokens(\n",
    "    tokenizer.encode(\"Hello, how are you?\"),\n",
    "    tokenizer.bos_token_id,\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.pad_token_id,\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "p_data = [re.sub(r'\\n\\[[\\x20-\\x7f]+\\]', \"\", row[2]) for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse538",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
