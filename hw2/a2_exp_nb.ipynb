{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"data/songs.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    data = [row for row in reader] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('openai-community/gpt2')\n",
    "tokenizer.bos_token = '<s>'\n",
    "tokenizer.eos_token = '</s>'\n",
    "tokenizer.pad_token = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "class TrigramLM:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerFast):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.vocab_size + 2 # for <s> and </s>\n",
    "        # TODO: check if this is correct and if we need to add <|endoftext|> to the vocab\n",
    "        self.unigram_counts = {}\n",
    "        self.bigram_counts = {}\n",
    "        self.trigram_counts = {}\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return self.tokenizer.convert_ids_to_tokens(self.tokenizer.encode(text))\n",
    "\n",
    "    def train(self, data: List[List[str]]) -> None:\n",
    "        tokenized_data = [self.tokenize(i) for i in data]\n",
    "\n",
    "        for row in tqdm(tokenized_data, desc=\"Counting\"):\n",
    "            row = [\"<s>\"] + row + [\"</s>\"]\n",
    "\n",
    "            for j,_ in enumerate(row):\n",
    "                self.unigram_counts[row[j]] = self.unigram_counts.get(row[j], 0) + 1\n",
    "\n",
    "                if j > 0:\n",
    "                    if row[j-1] not in self.bigram_counts:\n",
    "                        self.bigram_counts[row[j-1]] = {}\n",
    "                    self.bigram_counts[row[j-1]][row[j]] = self.bigram_counts[row[j-1]].get(row[j], 0) + 1\n",
    "\n",
    "                if j > 1:\n",
    "                    if row[j-2] not in self.trigram_counts:\n",
    "                        self.trigram_counts[row[j-2]] = {}\n",
    "                    if row[j-1] not in self.trigram_counts[row[j-2]]:\n",
    "                        self.trigram_counts[row[j-2]][row[j-1]] = {}\n",
    "                    self.trigram_counts[row[j-2]][row[j-1]][row[j]] = self.trigram_counts[row[j-2]][row[j-1]].get(row[j], 0) + 1\n",
    "        return None\n",
    "    \n",
    "    def add_one_smoothed_prob(self, n_counts: Union[int, List[int]], d_counts: int) -> Union[float, List[float]]:\n",
    "        if isinstance(n_counts, int):\n",
    "            return (n_counts + 1) / (d_counts + self.vocab_size)\n",
    "        else:\n",
    "            return [self.add_one_smoothed_prob(n, d_counts) for n in n_counts]\n",
    "\n",
    "    def nextProb(self, history_toks: List[str], next_toks: List[str]) -> float:\n",
    "        assert hasattr(history_toks, \"__len__\")\n",
    "        if len(history_toks) == 0:\n",
    "            n_counts = [self.unigram_counts.get(tok, 0) for tok in next_toks]\n",
    "            d_counts = self.bigram_count\n",
    "\n",
    "        elif len(history_toks) == 1:\n",
    "            prev_tok = history_toks[0]\n",
    "            n_counts = [self.bigram_counts.get(prev_tok, {}).get(tok, 0) for tok in next_toks]\n",
    "            d_counts = self.unigram_counts.get(prev_tok, 0)\n",
    "\n",
    "        else:\n",
    "            prev_tok1, prev_tok2 = history_toks[-2:]\n",
    "            n_counts = [self.trigram_counts.get(prev_tok1, {}).get(prev_tok2, {}).get(tok, 0) for tok in next_toks]\n",
    "            d_counts = self.bigram_counts.get(prev_tok1, {}).get(prev_tok2, 0)\n",
    "\n",
    "        return self.add_one_smoothed_prob(n_counts, d_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [tokenizer.convert_ids_to_tokens(tokenizer.encode(row[2])) for row in data]\n",
    "# TODO: check if there is an issue due to sequence length > 1024\n",
    "# TODO: check if custom newline handling is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel = TrigramLM(tokenizer)\n",
    "lmodel.train([i[2] for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|<endoftext>|\" in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.nextProb([\"<s>\", \"I\"], [\"Ġremember\", \"Ġwhen\", \"ĠI\", \"Ġwas\", \"Ġyoung\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([k for i, j in lmodel.trigram_counts.items() for m, n in j.items() for k in n.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lmodel.unigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(row[2]) for row in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "142982/490572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse538",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
