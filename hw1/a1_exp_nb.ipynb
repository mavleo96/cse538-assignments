{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "1. error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "# from a1_p1_murugan_116745378 import wordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConllTags(filename: str) -> List[List]:\n",
    "    # input: filename for a conll style parts of speech tagged file\n",
    "    # output: a list of list of tuples [sent]. representing [[[word1, tag], [word2, tag2]]\n",
    "\n",
    "    wordTagsPerSent = [[]]\n",
    "    sentNum = 0\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        for wordtag in f:\n",
    "            wordtag = wordtag.strip()\n",
    "            if wordtag:  # still reading current sentence\n",
    "                (word, tag) = wordtag.split(\"\\t\")\n",
    "                wordTagsPerSent[sentNum].append((word, tag))\n",
    "            else:  # new sentence\n",
    "                wordTagsPerSent.append([])\n",
    "                sentNum += 1\n",
    "    return wordTagsPerSent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getConllTags(\"data/daily547_3pos.txt\")\n",
    "\n",
    "with open(\"data/daily547_tweets.txt\", encoding=\"utf8\") as f:\n",
    "    tweets = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_PATTERN = r\"\"\"\n",
    "https?://\\S+\\.\\S+\\w\\/?|                              # URLs with http or https\n",
    "\\w+\\.com\\b|                                          # URLs with .com\n",
    "[:;]-?[\\)D\\(P/]|                                     # Emoticons 1\n",
    "[DP][:;]|                                            # Emoticons 2\n",
    "(?:[A-Z]\\.)+|                                        # Abbreviations\n",
    "[A-z]+[`'][A-z]+|                                    # Contractions\n",
    "\\d+\\.\\d+|                                            # Numbers with decimal\n",
    "\\d+:\\d+|                                             # Time\n",
    "# [$£]?(?:\\d{,3},)*\\d+(?:\\.\\d+)?|                    # Money\n",
    "\\w+[\\/]\\w+|                                          # Words with slashes\n",
    "(?:\\.+|,+|!+|\\?+|\\(+|\\)+|\\?\\!|[:;\"'`~\\{\\}\\[\\]])|     # Punctuation\n",
    "[@#]?[\\w\\-]+|                                        # Words with optional @ or #\n",
    "\\S                                                   # Any other non-whitespace character\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def wordTokenizer(sent: str) -> List[str]:\n",
    "    \"\"\"Split a string into list of tokens matched by regex\"\"\"\n",
    "    # TODO: Need to check if the regex is accurate enough\n",
    "    # TODO: A. should be captured as [\"A\", \".\"] and not [\"A.\"]\n",
    "    # pattern = re.compile(\n",
    "    #     r\"(?:[A-Z]\\.)+|[A-z]+'[A-z]+|\\d+\\.\\d+|[.,:;'`]|[@#]?[A-Za-z0-9]+|\\S+\"\n",
    "    # )\n",
    "    pattern = re.compile(REGEX_PATTERN, re.VERBOSE)\n",
    "\n",
    "    tokens = re.findall(pattern, sent)\n",
    "\n",
    "    # Check if tokens add back to original sentence\n",
    "    assert \"\".join(tokens) == \"\".join(\n",
    "        sent.split()\n",
    "    ), f\"Tokens don't add up to original sentence\\nTokens: {tokens}\\nSentence: {sent}\"\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the wordTokenizer function\n",
    "a = [[t for t,_ in s] for s in data ]\n",
    "b = [wordTokenizer(s) for s in tweets[:150]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokenizer(\"U.S.A. A.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in range(150):\n",
    "    if set(a[i]) != set(b[i]):\n",
    "        counter += 1\n",
    "        print(\"Error in tweet\", i)\n",
    "        print(\"a:\", a[i])\n",
    "        print(\"b:\", b[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of errors:\", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_postags = set.union(*[set(token[1] for token in sentence) for sentence in data])\n",
    "unique_tokens = set.union(*[set(token[0] for token in sentence) for sentence in data])\n",
    "\n",
    "postag_index = {postag: id for id, postag in enumerate(unique_postags)}\n",
    "token_index = {token: id for id, token in enumerate(unique_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeaturesForTarget(tokens: List[str], targetI: int, wordToIndex: Dict[str, int]) -> np.array:\n",
    "    # input: tokens: a list of tokens in a sentence,\n",
    "    #        targetI: index for the target token\n",
    "    #        wordToIndex: dict mapping ‘word’ to an index in the feature list.\n",
    "    # output: list (or np.array) of k feature values for the given target\n",
    "\n",
    "    #<FILL IN>\n",
    "    assert targetI < len(tokens), \"list index out of range\"\n",
    "\n",
    "    # feature 1\n",
    "    first_letter, fl_ascii = tokens[targetI][0], ord(tokens[targetI][0])\n",
    "    capital = np.array([int(64 < fl_ascii < 91)])\n",
    "\n",
    "    # feature 2\n",
    "    first_letter_f = np.zeros(257)\n",
    "    first_letter_f[fl_ascii if fl_ascii < 256 else 256] = 1\n",
    "    # fl_ascii = ord(tokens[targetI][0])\n",
    "\n",
    "    # feature 3\n",
    "    length = np.array([len(tokens[targetI])])\n",
    "\n",
    "    # feature 4\n",
    "    previous_word = np.zeros(len(wordToIndex))\n",
    "    if targetI != 0:\n",
    "        previous_word[wordToIndex[tokens[targetI - 1]]] = 1\n",
    "\n",
    "    # feature 5\n",
    "    current_word = np.zeros(len(wordToIndex))\n",
    "    current_word[wordToIndex[tokens[targetI]]] = 1\n",
    "\n",
    "    # feature 6\n",
    "    next_word = np.zeros(len(wordToIndex))\n",
    "    if targetI != len(tokens) - 1:\n",
    "        next_word[wordToIndex[tokens[targetI + 1]]] = 1\n",
    "\n",
    "    feature_vector = np.concatenate([\n",
    "        capital,\n",
    "        first_letter_f,\n",
    "        length,\n",
    "        previous_word,\n",
    "        current_word,\n",
    "        next_word,\n",
    "    ])\n",
    "\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    getFeaturesForTarget([i for i,_ in sentence], id, token_index)\n",
    "    for sentence in data\n",
    "    for id, _ in enumerate(sentence)\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    postag_index[postag]\n",
    "    for sentence in data\n",
    "    for _, postag in sentence\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0, :].sum(), *X[-5:, :].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MulticlassLogisticRegression(nn.Module):\n",
    "    def __init__(self, dim, nclass):\n",
    "        super(MulticlassLogisticRegression, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(dim, nclass, dtype=torch.float32)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.log_softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "model = MulticlassLogisticRegression(X.shape[1], 3)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "Xt = torch.tensor(X_train, dtype=torch.float32)\n",
    "yt = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "Xd = torch.tensor(X_dev, dtype=torch.float32)\n",
    "yd = torch.tensor(y_dev, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(Xt, yt)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=200)\n",
    "\n",
    "train_loss = []\n",
    "dev_loss = []\n",
    "train_accuracy = []\n",
    "dev_accuracy = []\n",
    "for epoch in range(200):\n",
    "    for batch_X, batch_y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        log_prob_pred = model(batch_X)\n",
    "        loss = loss_fn(log_prob_pred, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_log_prob_pred = model(Xt)\n",
    "    dev_log_prob_pred = model(Xd)\n",
    "    train_y_pred = train_log_prob_pred.argmax(1)\n",
    "    dev_y_pred = dev_log_prob_pred.argmax(1)\n",
    "\n",
    "    train_loss.append(loss_fn(train_log_prob_pred, yt).item())\n",
    "    dev_loss.append(loss_fn(dev_log_prob_pred, yd).item())\n",
    "    train_accuracy.append(accuracy_score(yt.numpy(), train_y_pred.numpy()))\n",
    "    dev_accuracy.append(accuracy_score(yd.numpy(), dev_y_pred.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a1_p1_murugan_116745378 import wordTokenizer\n",
    "from a1_p2_murugan_116745378 import getFeaturesForTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSentences = [\n",
    "    'The horse raced past the barn fell.',\n",
    "    'For 3 years, we attended S.B.U. in the CS program.',\n",
    "    'Did you hear Sam tell me to \"chill out\" yesterday? #rude'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sampleSentences[2]\n",
    "t = wordTokenizer(s)\n",
    "Xi = np.array([getFeaturesForTarget(t, i, token_index) for i in range(len(t))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor(Xi, dtype=torch.float32)).argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import heapq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacelessBPETokenize(text, vocab):\n",
    "    # input: text, a single string to be word tokenized.\n",
    "    #       vocab, a set of valid vocabulary words\n",
    "    # output: words, a list of strings of all word tokens, in order, from the string\n",
    "    words = None\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_set_to_prefix_tree(vocab):\n",
    "    # input: vocab, a set of strings\n",
    "    # output: prefix_tree, a dict of dicts representing a prefix tree\n",
    "    prefix_tree = None\n",
    "    return prefix_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"i\", \"n\", \"a\", \"b\", \"in\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heapq._heapify_max(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heapq.heapify(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heapq."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
