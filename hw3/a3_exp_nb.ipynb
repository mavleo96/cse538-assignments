{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, re, collections, string\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import csv\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import heapq\n",
    "import matplotlib\n",
    "import tqdm\n",
    "import transformers\n",
    "import datasets\n",
    "import sentence_transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoModelForCausalLM, RobertaTokenizer, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ids = list(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(correct_ids, 5, replace=False).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.random.randint(0, 2, (1, 100)).reshape(-1)\n",
    "pred = np.random.randint(0, 2, (1, 100)).reshape(-1)\n",
    "# pred = np.full((1, 100), 0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true, pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"acc: {accuracy_score(true, pred):.3f}, f1: {f1_score(true, pred, average=\"macro\"):.3f}\n",
    "Yes: prec: {precision_score(true, pred, pos_label=1, zero_division=0):.3f}, recall: {recall_score(true, pred, pos_label=1, zero_division=0):.3f}, f1: {f1_score(true, pred, pos_label=1, zero_division=0):.3f}\n",
    "No: prec: {precision_score(true, pred, pos_label=0, zero_division=0):.3f}, recall: {recall_score(true, pred, pos_label=0, zero_division=0):.3f}, f1: {f1_score(true, pred, pos_label=0, zero_division=0):.3f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 500, 100)\n",
    "target = torch.randint(0, 500, (1, 100))\n",
    "F.cross_entropy(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "input = torch.randn(3, 2, requires_grad=True)\n",
    "target = torch.rand(3, 2, requires_grad=False)\n",
    "output = F.binary_cross_entropy(m(input), target, reduction='none')\n",
    "output, input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "outputs = torch.sigmoid(model(inputs))  # shape: [batch_size]\n",
    "loss = criterion(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dummy input: 3 samples, 1 output neuron (binary classification)\n",
    "logits = torch.tensor([[0.2], [1.0], [-1.0]], dtype=torch.float32)\n",
    "targets = torch.tensor([[0.0], [1.0], [0.0]], dtype=torch.float32)  # labels\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = torch.sigmoid(logits)\n",
    "\n",
    "# Use BCELoss\n",
    "bce_loss = nn.BCELoss()\n",
    "loss = bce_loss(probs, targets)\n",
    "\n",
    "print(f\"BCELoss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 500, 100)\n",
    "target = torch.tensor(torch.randint(0, 500, (1, 100))\n",
    "# F.cross_entropy(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parts I and II\n",
    "boolq_dataset = load_dataset('google/boolq')\n",
    "\n",
    "boolq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = [1 if x[\"answer\"] else 0 for x in boolq_dataset[\"validation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, RobertaTokenizer, AutoModelForCausalLM, RobertaForMaskedLM\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "# model = GPT2LMHeadModel.from_pretrained('distilgpt2').cuda()\n",
    "# model = AutoModelForCausalLM.from_pretrained('distilgpt2').cuda()\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "model = RobertaForMaskedLM.from_pretrained('distilroberta-base')#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head = nn.Sequential(\n",
    "    nn.Linear(model.lm_head.dense.in_features, 1),\n",
    "    nn.Sigmoid()\n",
    ")#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(0, tokenizer.vocab_size, (5, 256)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.logits#[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(0, 2, (1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolq2tensor(x, append_answer=False):\n",
    "    if append_answer:\n",
    "        return tokenizer.encode(f\"{x['passage']}.\\n{x['question']}?\\n{'yes' if x['answer'] else 'no'}\", return_tensors=\"pt\")\n",
    "    else:\n",
    "        return tokenizer.encode(f\"{x['passage']}.\\n{x['question']}?\\n\", return_tensors=\"pt\")\n",
    "# List is alright here for the moment since max token length is 1024 and the max in the dataset is 25\n",
    "val_set = [boolq2tensor(x) for x in boolq_dataset[\"validation\"]]\n",
    "train_set = [boolq2tensor(x, append_answer=True) for x in boolq_dataset[\"train\"]]\n",
    "train_set2 = [boolq2tensor(x) for x in boolq_dataset[\"train\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_len = pd.Series(val_set).apply(lambda x: x.shape[1])\n",
    "train_len = pd.Series(train_set).apply(lambda x: x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_len.hist(bins=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorlist2padded(tensorlist, length: int, pad_token_id: int, pad_strategy: str):\n",
    "    assert len(tensorlist) > 0\n",
    "    assert all(x.ndim == 2 for x in tensorlist)\n",
    "    assert all(x.shape[0] == 1 for x in tensorlist)\n",
    "    assert length > 0\n",
    "    assert pad_token_id is not None\n",
    "    assert pad_strategy in {\"left\", \"right\"}\n",
    "\n",
    "    for i, x in enumerate(tensorlist):\n",
    "        if x.shape[1] < length:\n",
    "            if pad_strategy == \"left\":\n",
    "                tensorlist[i] = torch.cat([torch.full((1, length - x.shape[1]), pad_token_id), x], dim=1)\n",
    "            else:\n",
    "                tensorlist[i] = torch.cat([x, torch.full((1, length - x.shape[1]), pad_token_id)], dim=1)\n",
    "        elif x.shape[1] > length:\n",
    "            if pad_strategy == \"left\":\n",
    "                tensorlist[i] = x[:, -length:]\n",
    "            else:\n",
    "                tensorlist[i] = x[:, :length]\n",
    "        else:\n",
    "            assert x.shape[1] == length\n",
    "\n",
    "    concat_tensor = torch.cat(tensorlist, dim=0)\n",
    "    return concat_tensor\n",
    "\n",
    "# val_tensor = tensorlist2padded(val_set, 200, tokenizer.unk_token_id, \"left\").cuda()\n",
    "# train_tensor = tensorlist2padded(train_set, 1024, tokenizer.unk_token_id, \"left\").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_tensor, batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(val_tensor, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = []\n",
    "# for x in val_set:\n",
    "for x in tqdm.tqdm(val_set):\n",
    "    with torch.no_grad():\n",
    "        if x.shape[1] > 1024:\n",
    "            x = x[:, -1024:]\n",
    "        out = model(x.cuda()).logits\n",
    "        # labels_pred.extend(out[:, -1, [645, 3763]].argmax(dim=1).cpu().tolist())\n",
    "        labels_pred.extend(out[:, -1, [3919, 8505]].argmax(dim=1).cpu().tolist())\n",
    "\n",
    "# tokenizer.decode(out[:, -1, :].argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_pred = []\n",
    "# for x in val_loader:\n",
    "#     with torch.no_grad():\n",
    "#         out = model(x.cuda()).logits\n",
    "#         labels_pred.extend(out[:, -1, [645, 3763]].argmax(dim=1).cpu().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1 if x['answer'] else 0 for x in boolq_dataset[\"validation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(labels_pred), Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode([\"no\", \"yes\"]) -> [3919, 8505]\n",
    "output_stacked = torch.stack(outputs)[:, [645, 3763]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1 if x['answer'] else 0 for x in boolq_dataset[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = output_stacked.argmax(dim=1)\n",
    "label_pred = label_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Overall: acc: {accuracy_score(labels, label_pred):.3f}, f1: {f1_score(labels, label_pred):.3f}\n",
    "    Yes: prec: {precision_score(labels, label_pred, pos_label=1):.3f}, recall: {recall_score(labels, label_pred, pos_label=1):.3f}, f1: {f1_score(labels, label_pred, pos_label=1):.3f}\n",
    "     No: prec: {precision_score(labels, label_pred, pos_label=0):.3f}, recall: {recall_score(labels, label_pred, pos_label=0):.3f}, f1: {f1_score(labels, label_pred, pos_label=0):.3f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilroberta-base\", is_decoder=True)\n",
    "model.lm_head = nn.Sequential(\n",
    "    nn.Linear(model.lm_head.dense.in_features, 1), nn.Sigmoid()\n",
    ")\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(0, 50256, (5, 256)).cuda()\n",
    "out = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.logits[:, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distilroberta_rand():\n",
    "    model = AutoModelForCausalLM.from_pretrained('distilroberta-base')\n",
    "    for name, module in model.named_modules():\n",
    "        # TODO: check if this is correct or this is to be done for all layers\n",
    "        if \"roberta.encoder.layer.4\" in name or \"roberta.encoder.layer.5\" in name:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight)\n",
    "                nn.init.normal_(module.bias)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distilroberta_kqv():\n",
    "    model = AutoModelForCausalLM.from_pretrained('distilroberta-base')\n",
    "    for name, module in model.named_modules():\n",
    "        if name in {\"roberta.encoder.layer.4\", \"roberta.encoder.layer.5\"}:\n",
    "            w = (module.attention.self.key.weight + module.attention.self.query.weight) / 2\n",
    "            b = (module.attention.self.key.bias + module.attention.self.query.bias) / 2\n",
    "            shared_linear_layer = nn.Linear(w.shape[1], w.shape[0])\n",
    "            with torch.no_grad():\n",
    "                shared_linear_layer.weight.copy_(w)\n",
    "                shared_linear_layer.bias.copy_(b)\n",
    "            module.attention.self.key = shared_linear_layer\n",
    "            module.attention.self.query = shared_linear_layer\n",
    "            module.attention.self.value = shared_linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from transformers.models.bert.modeling_bert.BertOutput\n",
    "class RobertaOutputNoRes(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class RobertaSelfOutputNoRes(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "def get_distilroberta_nores():\n",
    "    model = AutoModelForCausalLM.from_pretrained('distilroberta-base')\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if name in {\"roberta.encoder.layer.4\", \"roberta.encoder.layer.5\"}:\n",
    "            nores_self_output = RobertaSelfOutputNoRes(model.config)\n",
    "            nores_output = RobertaOutputNoRes(model.config)\n",
    "\n",
    "            nores_self_output.load_state_dict(module.attention.output.state_dict())\n",
    "            nores_output.load_state_dict(module.output.state_dict())\n",
    "\n",
    "            module.attention.output = nores_self_output\n",
    "            module.output = nores_output\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = get_distilroberta_nores().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(0, tokenizer.vocab_size, (5, 256)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3(input).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_dataset = load_dataset('stanfordnlp/sst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['label'] for x in sst_dataset[\"train\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['sentence'] for x in sst_dataset[\"train\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaForCausalLM, RobertaForMaskedLM, AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "model = RobertaModel.from_pretrained('distilroberta-base')\n",
    "# model = RobertaForCausalLM.from_pretrained('distilroberta-base', is_decoder=False)\n",
    "# model = RobertaForMaskedLM.from_pretrained('distilroberta-base')\n",
    "# model.pooler.dense = nn.Linear(model.config.hidden_size, 1)\n",
    "# model.pooler.activation = nn.Identity() #nn.Sigmoid()\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaModel.from_pretrained(\"distilroberta-base\")\n",
    "model.pooler.dense = nn.Linear(model.pooler.dense.in_features, 1)\n",
    "# TODO: autocast not compatible with BCE loss\n",
    "# model.pooler.activation = nn.Identity()\n",
    "model.pooler.activation = nn.Sigmoid()\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('distilroberta-base', num_labels=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(0, tokenizer.vocab_size, (5, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.randint(0, 2, (5, 512))\n",
    "mask = mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(input, attention_mask=mask).pooler_output#.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [tokenizer.encode(x['sentence']) for x in sst_dataset['train']]\n",
    "b = [x['label'] for x in sst_dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.extend(b.cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(b).hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([len(x) for x in a]).hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_unique_ctx_examples(squad, n=500):\n",
    "    context2idx = {}\n",
    "    for i, entry in enumerate(squad['validation']):\n",
    "        if not entry['context'] in context2idx:\n",
    "            context2idx[entry['context']] = []\n",
    "        context2idx[entry['context']].append(i)\n",
    "\n",
    "    queries, contexts, answers = [], [], []\n",
    "    for k,v in context2idx.items():\n",
    "        idx = v[0]\n",
    "        queries.append(squad['validation'][idx]['question'])\n",
    "        contexts.append(squad['validation'][idx]['context'])\n",
    "        answers.append(squad['validation'][idx]['answers'])\n",
    "        if len(queries) == n:\n",
    "            break\n",
    "    return queries, contexts, answers\n",
    "\n",
    "squad = load_dataset(\"squad\")\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "queries, contexts, answers = get_unique_ctx_examples(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[333]['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = sentence_model.encode(contexts)\n",
    "query_embedding = sentence_model.encode(queries[333]).reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(embeddings, query_embedding).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(contexts, embeddings, query):\n",
    "    # inputs\n",
    "    #     contexts: list of val500 context strings\n",
    "    #     embeddings: list/array/tensor of vector embedding of each context string\n",
    "    #     query: a single question as a string\n",
    "    # outputs\n",
    "    #     idx: index of context vector with highest cosine similarity with query\n",
    "    #     ret_context: retrieved context\n",
    "\n",
    "    query_embedding = sentence_model.encode(query).reshape(1, -1)\n",
    "    similarities = cosine_similarity(embeddings, query_embedding)\n",
    "    idx = similarities.argmax()\n",
    "    ret_context = contexts[idx]\n",
    "\n",
    "    return idx, ret_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = queries[42]\n",
    "\n",
    "idx, retrieved_text = retrieve(contexts, embeddings, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_text, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. \"\n",
    "            \"Provide one Answer ONLY to the following query based on the context provided below. \"\n",
    "            \"Do not generate or answer any other questions. \"\n",
    "            \"Do not make up or infer any information that is not directly stated in the context. \"\n",
    "            \"Provide a concise answer / answer in one word\"\n",
    "            f\"\\nContext: {retrieved_text}\"},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.amp.autocast(dtype=torch.bfloat16, device_type=\"cuda\"):\n",
    "    output = model.generate(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.split(\"<|assistant|>\")[-1].split(\"<|end|>\")[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, query, ret_context):\n",
    "    # input\n",
    "    #     model: an instance of LM\n",
    "    #     query: the question as a string\n",
    "    #     ret_context: context retrieved from the embedded vectors\n",
    "    # output\n",
    "    #     response: a string of tokens obtained from the model\n",
    "\n",
    "    #<FILL IN>\n",
    "\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "torch.random.manual_seed(0) \n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    # device_map=\"cuda\",  \n",
    "    # torch_dtype=\"auto\",  \n",
    "    # trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") \n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, \n",
    "] \n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse538",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
